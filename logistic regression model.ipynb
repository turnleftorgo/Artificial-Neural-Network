{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaRo3B5tvD+YSmcM0M5q8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/turnleftorgo/Artificial-Neural-Network/blob/main/logistic%20regression%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hR8XgbNNZCdm",
        "outputId": "9a262d02-afb1-4a3b-9492-80b6be7066a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
        "from sklearn.model_selection import train_test_split  # for train/test split\n",
        "\n",
        "\"\"\"\n",
        "torch is Pytorch : A powerful and flexible deep learning framework that allows you to build and train neural networks.\n",
        "\n",
        "torch.nn : Pytorch Neural network module\n",
        "\n",
        "NumPy: A fundamental package for scientific computing in Python. like Array Operations , Provides support for large,\n",
        "multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "\n",
        "StandardScaler ensures that the features have similar scales, which is important for algorithms that are sensitive to the scales of input data,\n",
        "such as gradient descent-based methods.\n",
        "\n",
        "sklearn : a open-source machine learning library\n",
        "\"\"\"\n",
        "\n",
        "# Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f'number of samples: {n_samples}, number of features: {n_features}')\n",
        "\n",
        "# split data to 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale data\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\"\"\"\n",
        "Many machine learning algorithms perform better when features are on a similar scale.\n",
        "This is especially true for algorithms that rely on gradient descent, like neural networks.\n",
        "\"\"\"\n",
        "\n",
        "# convert to tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.int64))\n",
        "y_test = torch.from_numpy(y_test.astype(np.int64))\n",
        "\n",
        "\"\"\"\n",
        "PyTorch uses tensors as its fundamental data structure, similar to how NumPy uses arrays.\n",
        "Tensors are required for creating and training neural networks in PyTorch.\n",
        "Tensors are similar to NumPy arrays but come with additional functionalities like GPU support and automatic differentiation,\n",
        "which are crucial for deep learning.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"Ensures that the data type of X_train is float32. Converts the NumPy array to a PyTorch tensor.\"\"\"\n",
        "\n",
        "\"\"\"classification targets are usually represented as integers, and PyTorch's loss functions expect the target labels to be in this format.\"\"\"\n",
        "\n",
        "# Create model\n",
        "# f = wx + b, softmax at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        pred_probs = torch.nn.Softmax(dim=-1)(logits) #for asking question only\n",
        "        return logits #return the logits\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LogisticRegression(n_features).to(device)  #load the model to the current device\n",
        "\n",
        "# Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass and loss\n",
        "    X_train, y_train = X_train.to(device), y_train.to(device) #load the data to device (GPU or CPU)\n",
        "    logits = model(X_train)\n",
        "\n",
        "    loss = loss_fn(logits, y_train.squeeze().long())\n",
        "\n",
        "    # backward pass to compute the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # updates the model parameter based on the gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ]
    }
  ]
}